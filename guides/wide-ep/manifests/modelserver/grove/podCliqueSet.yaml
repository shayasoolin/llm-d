# FP4 + NVIDIA MNNVL Configuration:
#   - VLLM_ALL2ALL_BACKEND=flashinfer_all2allv (native MNNVL support via MnnvlMoe)
#   - VLLM_USE_FLASHINFER_MOE_FP4=1 (enable FlashInfer FP4 MoE kernels)
#   - VLLM_FLASHINFER_MOE_BACKEND=throughput (use CUTLASS backend with alltoallv integration)
#
# NOTE: EPLB (--enable-eplb) is NOT supported with FP4, do not add it.
apiVersion: grove.io/v1alpha1
kind: PodCliqueSet
metadata:
  name: llm-d-wide-ep
spec:
  replicas: 1
  template:
    cliques:
    - name: prefill
      labels:
        kai.scheduler/queue: default-queue
        llm-d.ai/inferenceServing: "true"
        llm-d.ai/model: DeepSeek-R1
        llm-d.ai/role: prefill
      spec:
        replicas: 1
        minAvailable: 1
        roleName: prefill
        podSpec:
          serviceAccountName: deepseek-r1
          # Resource claims for ComputeDomain (MNNVL)
          resourceClaims:
            - name: compute-domain-channel
              resourceClaimTemplateName: llm-d-wide-ep-compute-domain-channel
          volumes:
            - name: dshm
              emptyDir:
                medium: Memory
                sizeLimit: 2Gi # roughly 32MB per local DP plus scratch space
          containers:
          - name: vllm
            image: ghcr.io/llm-d/llm-d-cuda:v0.4.0
            securityContext:
              capabilities:
                add:
                - IPC_LOCK
                - SYS_RAWIO
              runAsGroup: 0
              runAsUser: 0
            imagePullPolicy: Always
            command:
              - /bin/bash
              - -c
            args:
              - |-
                # Clear /dev/shm on start to prevent running out of space when crashes occur
                # https://github.com/llm-d/llm-d/issues/352
                find /dev/shm -type f -delete

                #################
                # RUN vLLM prefill
                #################

                exec vllm serve \
                  nvidia/DeepSeek-R1-NVFP4 \
                  --port 8000 \
                  --trust-remote-code \
                  --disable-uvicorn-access-log \
                  --tensor-parallel-size $TP_SIZE \
                  --kv_transfer_config '{"kv_connector":"NixlConnector",
                                          "kv_role":"kv_both"}' \
            env:
            - name: TP_SIZE
              value: "4"
            # MNNVL (Multi-Node NVLink) settings for KV cache transfer
            - name: NCCL_MNNVL_ENABLE
              value: "1"
            - name: NCCL_CUMEM_ENABLE
              value: "1"
            - name: GLOO_SOCKET_IFNAME
              value: eth0
            - name: NCCL_SOCKET_IFNAME
              value: eth0
            - name: TORCH_DISTRIBUTED_TIMEOUT
              value: "7200"  # 2 hours for multi-node distributed init
            - name: NCCL_STORE_TIMEOUT
              value: "7200"  # 2 hours for NCCL store operations
            - name: TORCH_DIST_INIT_BARRIER_TIMEOUT
              value: "7200"  # 2 hours for barrier timeout
            - name: VLLM_NIXL_SIDE_CHANNEL_HOST
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            ports:
            - containerPort: 8000
              name: metrics
              protocol: TCP
            startupProbe:
              httpGet:
                path: /health
                port: metrics
              initialDelaySeconds: 0
              periodSeconds: 10
              timeoutSeconds: 5
              failureThreshold: 1800
            livenessProbe:
              httpGet:
                path: /health
                port: metrics
              periodSeconds: 30
              timeoutSeconds: 5
              failureThreshold: 3
            readinessProbe:
              httpGet:
                path: /v1/models
                port: metrics
              periodSeconds: 10
              timeoutSeconds: 5
              failureThreshold: 3
            resources:
              claims:
                - name: compute-domain-channel
              limits:
                memory: 512Gi
                nvidia.com/gpu: "4"
              requests:
                cpu: 32
                memory: 512Gi
                nvidia.com/gpu: "4"
            volumeMounts:
              - name: dshm
                mountPath: /dev/shm
    - name: decode-leader
      labels:
        kai.scheduler/queue: default-queue
        llm-d.ai/inferenceServing: "true"
        llm-d.ai/model: DeepSeek-R1
        llm-d.ai/role: decode
      spec:
        replicas: 1
        minAvailable: 1
        roleName: decode-leader
        podSpec:
          serviceAccountName: deepseek-r1
          # Resource claims for ComputeDomain (MNNVL)
          resourceClaims:
            - name: compute-domain-channel
              resourceClaimTemplateName: llm-d-wide-ep-compute-domain-channel
          initContainers:
            - name: routing-proxy
              args:
                - --port=8000
                - --vllm-port=8200
                - --connector=nixlv2
                - -v=1
                - --secure-proxy=false
              image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.3.0
              imagePullPolicy: Always
              ports:
                - containerPort: 8000
                  name: sidecar
                  protocol: TCP
              resources: {}
              restartPolicy: Always
              securityContext:
                allowPrivilegeEscalation: false
                runAsNonRoot: true
          volumes:
            - name: dshm
              emptyDir:
                medium: Memory
                sizeLimit: 2Gi # roughly 32MB per local DP plus scratch space
          containers:
          - name: vllm
            image: ghcr.io/llm-d/llm-d-cuda:v0.4.0
            securityContext:
              capabilities:
                add:
                - IPC_LOCK
                - SYS_RAWIO
              runAsGroup: 0
              runAsUser: 0
            imagePullPolicy: Always
            command:
              - /bin/bash
              - -c
            args:
              - |-
                # Clear /dev/shm on start to prevent running out of space when crashes occur
                # https://github.com/llm-d/llm-d/issues/352
                find /dev/shm -type f -delete

                #################
                # RUN vLLM decode worker
                #################
                START_RANK=0

                LEADER_PCLQ_NAME="${GROVE_PCLQ_NAME/worker/leader}"
                LEADER_ADDRESS="${LEADER_PCLQ_NAME}-0.${GROVE_HEADLESS_SERVICE}"

              
                # --data-parallel-hybrid-lb: Use exernal load balancing across nodes, and internal load balancing within a node
                # --enable-expert-parallel:  Use TPxDP in attention, EP in MoE layers
                # --async-scheduling: Reduce white space between engine steps
                # --enable-eplb: Expert-parallel load balancing reduces EP load imbalance by replicating heavily-used experts Performance-memory tradeoff: on DeepSeekV3 eplb uses an extra 2GB per redundant expert per GPU Divisibility constraint: num_routed_experts (256 for DSv3) + num_redundant_experts must be divisible by the number of GPUs.
                # NOTE: DBO (--enable-dbo) is NOT compatible with flashinfer_all2allv backend required for MNNVL

                exec vllm serve \
                  nvidia/DeepSeek-R1-NVFP4 \
                  --port 8200 \
                  --trust-remote-code \
                  --disable-uvicorn-access-log \
                  --data-parallel-hybrid-lb \
                  --enable-expert-parallel \
                  --tensor-parallel-size $TP_SIZE \
                  --data-parallel-size $((GROVE_PCSG_TEMPLATE_NUM_PODS * DP_SIZE_LOCAL)) \
                  --data-parallel-size-local $DP_SIZE_LOCAL \
                  --data-parallel-address ${LEADER_ADDRESS} \
                  --data-parallel-rpc-port 5555 \
                  --data-parallel-start-rank $START_RANK \
                  --kv_transfer_config '{"kv_connector":"NixlConnector",
                                          "kv_role":"kv_both"}' \
                  --async-scheduling \
                  --compilation_config '{"cudagraph_mode": "FULL_DECODE_ONLY"}' \
                  --kv-cache-memory-bytes=${KV_CACHE_MEMORY_BYTES-}
            env:
            # VLLM_MOE_DP_CHUNK_SIZE is a tunable parameter.
            # Higher values increase the memory footprint of the hidden states in the MoE layers,
            # which decreases the available memory for KV Cache.
            # Lower values may cause poor performance, especially in load-imbalanced scenarios.
            # The value 384 was chosen to be greater than the concurrency expected to see on any DP rank.
            # If you increase the VLLM_MOE_DP_CHUNK_SIZE above 510 you will also need to increase the
            # default NVSHMEM_QP_DEPTH to be at least 2 * (chunk_size + 1), which will increase the
            # memory NVSHMEM allocates up front.
            # Has no effect when VLLM_ALL2ALL_BACKEND=deepep_high_throughput.
            - name: VLLM_MOE_DP_CHUNK_SIZE
              value: "384" # vLLM default is 256
            - name: DP_SIZE_LOCAL
              value: "4"
            - name: TP_SIZE
              value: "1"
            - name: TRITON_LIBCUDA_PATH
              value: /usr/lib64
            - name: VLLM_SKIP_P2P_CHECK
              value: "1"
            - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
              value: "1"
            - name: VLLM_USE_DEEP_GEMM
              value: "1"
            - name: VLLM_ALL2ALL_BACKEND
              value: flashinfer_all2allv
            # FlashInfer FP4 MoE settings for MNNVL
            - name: VLLM_USE_FLASHINFER_MOE_FP4
              value: "1"
            - name: VLLM_FLASHINFER_MOE_BACKEND
              value: throughput
            # MNNVL (Multi-Node NVLink) settings
            - name: NCCL_MNNVL_ENABLE
              value: "1"
            - name: NCCL_CUMEM_ENABLE
              value: "1"
            - name: GLOO_SOCKET_IFNAME
              value: eth0
            - name: NCCL_SOCKET_IFNAME
              value: eth0
            - name: TORCH_DISTRIBUTED_TIMEOUT
              value: "7200"  # 2 hours for multi-node distributed init
            - name: NCCL_STORE_TIMEOUT
              value: "7200"  # 2 hours for NCCL store operations
            - name: TORCH_DIST_INIT_BARRIER_TIMEOUT
              value: "7200"  # 2 hours for barrier timeout
            - name: VLLM_NIXL_SIDE_CHANNEL_HOST
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            ports:
            - containerPort: 8200
              name: metrics
              protocol: TCP
            startupProbe:
              httpGet:
                path: /health
                port: metrics
              initialDelaySeconds: 0
              periodSeconds: 10
              timeoutSeconds: 5
              failureThreshold: 1800
            livenessProbe:
              httpGet:
                path: /health
                port: metrics
              periodSeconds: 30
              timeoutSeconds: 5
              failureThreshold: 3
            readinessProbe:
              httpGet:
                path: /v1/models
                port: metrics
              periodSeconds: 10
              timeoutSeconds: 5
              failureThreshold: 3
            resources:
              claims:
                - name: compute-domain-channel
              limits:
                memory: 512Gi
                nvidia.com/gpu: "4"
              requests:
                cpu: 32
                memory: 512Gi
                nvidia.com/gpu: "4"
            volumeMounts:
              - name: dshm
                mountPath: /dev/shm
    - name: decode-worker
      labels:
        kai.scheduler/queue: default-queue
        llm-d.ai/inferenceServing: "true"
        llm-d.ai/model: DeepSeek-R1
        llm-d.ai/role: decode
      spec:
        replicas: 7
        minAvailable: 7
        roleName: decode-worker
        podSpec:
          serviceAccountName: deepseek-r1
          # Resource claims for ComputeDomain (MNNVL)
          resourceClaims:
            - name: compute-domain-channel
              resourceClaimTemplateName: llm-d-wide-ep-compute-domain-channel
          initContainers:
            - name: routing-proxy
              args:
                - --port=8000
                - --vllm-port=8200
                - --connector=nixlv2
                - -v=1
                - --secure-proxy=false
              image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.3.0
              imagePullPolicy: Always
              ports:
                - containerPort: 8000
                  name: sidecar
                  protocol: TCP
              resources: {}
              restartPolicy: Always
              securityContext:
                allowPrivilegeEscalation: false
                runAsNonRoot: true
          volumes:
            - name: dshm
              emptyDir:
                medium: Memory
                sizeLimit: 2Gi # roughly 32MB per local DP plus scratch space
          containers:
          - name: vllm
            image: ghcr.io/llm-d/llm-d-cuda:v0.4.0
            securityContext:
              capabilities:
                add:
                - IPC_LOCK
                - SYS_RAWIO
              runAsGroup: 0
              runAsUser: 0
            imagePullPolicy: Always
            command:
              - /bin/bash
              - -c
            args:
              - |-
                # Clear /dev/shm on start to prevent running out of space when crashes occur
                # https://github.com/llm-d/llm-d/issues/352
                find /dev/shm -type f -delete

                #################
                # RUN vLLM decode worker
                #################
                START_RANK=$(( ( ${GROVE_PCLQ_POD_INDEX:-0} + 1 ) * DP_SIZE_LOCAL ))

                LEADER_PCLQ_NAME="${GROVE_PCLQ_NAME/worker/leader}"
                LEADER_ADDRESS="${LEADER_PCLQ_NAME}-0.${GROVE_HEADLESS_SERVICE}"
              
                # --data-parallel-hybrid-lb: Use exernal load balancing across nodes, and internal load balancing within a node
                # --enable-expert-parallel:  Use TPxDP in attention, EP in MoE layers
                # --async-scheduling: Reduce white space between engine steps
                # --enable-eplb: Expert-parallel load balancing reduces EP load imbalance by replicating heavily-used experts Performance-memory tradeoff: on DeepSeekV3 eplb uses an extra 2GB per redundant expert per GPU Divisibility constraint: num_routed_experts (256 for DSv3) + num_redundant_experts must be divisible by the number of GPUs.
                # NOTE: DBO (--enable-dbo) is NOT compatible with flashinfer_all2allv backend required for MNNVL

                exec vllm serve \
                  nvidia/DeepSeek-R1-NVFP4 \
                  --port 8200 \
                  --trust-remote-code \
                  --disable-uvicorn-access-log \
                  --data-parallel-hybrid-lb \
                  --enable-expert-parallel \
                  --tensor-parallel-size $TP_SIZE \
                  --data-parallel-size $((GROVE_PCSG_TEMPLATE_NUM_PODS * DP_SIZE_LOCAL)) \
                  --data-parallel-size-local $DP_SIZE_LOCAL \
                  --data-parallel-address ${LEADER_ADDRESS} \
                  --data-parallel-rpc-port 5555 \
                  --data-parallel-start-rank $START_RANK \
                  --kv_transfer_config '{"kv_connector":"NixlConnector",
                                          "kv_role":"kv_both"}' \
                  --async-scheduling \
                  --compilation_config '{"cudagraph_mode": "FULL_DECODE_ONLY"}' \
                  --kv-cache-memory-bytes=${KV_CACHE_MEMORY_BYTES-}
            env:
            # VLLM_MOE_DP_CHUNK_SIZE is a tunable parameter.
            # Higher values increase the memory footprint of the hidden states in the MoE layers,
            # which decreases the available memory for KV Cache.
            # Lower values may cause poor performance, especially in load-imbalanced scenarios.
            # The value 384 was chosen to be greater than the concurrency expected to see on any DP rank.
            # If you increase the VLLM_MOE_DP_CHUNK_SIZE above 510 you will also need to increase the
            # default NVSHMEM_QP_DEPTH to be at least 2 * (chunk_size + 1), which will increase the
            # memory NVSHMEM allocates up front.
            # Has no effect when VLLM_ALL2ALL_BACKEND=deepep_high_throughput.
            - name: VLLM_MOE_DP_CHUNK_SIZE
              value: "384" # vLLM default is 256
            - name: DP_SIZE_LOCAL
              value: "4"
            - name: TP_SIZE
              value: "1"
            - name: TRITON_LIBCUDA_PATH
              value: /usr/lib64
            - name: VLLM_SKIP_P2P_CHECK
              value: "1"
            - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
              value: "1"
            - name: VLLM_USE_DEEP_GEMM
              value: "1"
            - name: VLLM_ALL2ALL_BACKEND
              value: flashinfer_all2allv
            # FlashInfer FP4 MoE settings for MNNVL
            - name: VLLM_USE_FLASHINFER_MOE_FP4
              value: "1"
            - name: VLLM_FLASHINFER_MOE_BACKEND
              value: throughput
            # MNNVL (Multi-Node NVLink) settings
            - name: NCCL_MNNVL_ENABLE
              value: "1"
            - name: NCCL_CUMEM_ENABLE
              value: "1"
            - name: GLOO_SOCKET_IFNAME
              value: eth0
            - name: NCCL_SOCKET_IFNAME
              value: eth0
            - name: TORCH_DISTRIBUTED_TIMEOUT
              value: "7200"  # 2 hours for multi-node distributed init
            - name: NCCL_STORE_TIMEOUT
              value: "7200"  # 2 hours for NCCL store operations
            - name: TORCH_DIST_INIT_BARRIER_TIMEOUT
              value: "7200"  # 2 hours for barrier timeout
            - name: VLLM_NIXL_SIDE_CHANNEL_HOST
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            ports:
            - containerPort: 8200
              name: metrics
              protocol: TCP
            startupProbe:
              httpGet:
                path: /health
                port: metrics
              initialDelaySeconds: 0
              periodSeconds: 10
              timeoutSeconds: 5
              failureThreshold: 1800
            livenessProbe:
              httpGet:
                path: /health
                port: metrics
              periodSeconds: 30
              timeoutSeconds: 5
              failureThreshold: 3
            readinessProbe:
              httpGet:
                path: /v1/models
                port: metrics
              periodSeconds: 10
              timeoutSeconds: 5
              failureThreshold: 3
            resources:
              claims:
                - name: compute-domain-channel
              limits:
                memory: 512Gi
                nvidia.com/gpu: "4"
              requests:
                cpu: 32
                memory: 512Gi
                nvidia.com/gpu: "4"
            volumeMounts:
              - name: dshm
                mountPath: /dev/shm
    podCliqueScalingGroups:
    - name: decode-sg
      cliqueNames: [decode-leader, decode-worker]
      replicas: 1
      minAvailable: 1