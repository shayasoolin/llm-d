# FP4 + NVIDIA MNNVL Configuration:
#   - VLLM_ALL2ALL_BACKEND=flashinfer_all2allv (native MNNVL support via MnnvlMoe)
#   - VLLM_USE_FLASHINFER_MOE_FP4=1 (enable FlashInfer FP4 MoE kernels)
#   - VLLM_FLASHINFER_MOE_BACKEND=throughput (use CUTLASS backend with alltoallv integration)
#
# NOTE: EPLB (--enable-eplb) is NOT supported with FP4, do not add it.

apiVersion: grove.io/v1alpha1
kind: PodCliqueSet
metadata:
  name: llm-d-wide-ep
spec:
  replicas: 1
  template:
    topologyConstraint:
      packDomain: rack
    cliques:
    - name: prefill
      labels:
        kai.scheduler/queue: default-queue
        llm-d.ai/inference-serving: "true"
        llm-d.ai/guide: "wide-ep"
        llm-d.ai/accelerator-variant: "gpu"
        llm-d.ai/accelerator-vendor: "nvidia"
        llm-d.ai/model: DeepSeek-R1-0528
        llm-d.ai/role: prefill
      spec:
        replicas: 1
        minAvailable: 1
        roleName: prefill
        podSpec:
          serviceAccountName: deepseek-r1
          schedulerName: kai-scheduler
          volumes:
            - name: dshm
              emptyDir:
                medium: Memory
                sizeLimit: 2Gi
            - name: hf-cache
              emptyDir: {}
            - name: jit-cache
              emptyDir: {}
          containers:
          - name: vllm
            image: ghcr.io/llm-d/llm-d-cuda:v0.5.0
            securityContext:
              capabilities:
                add:
                - IPC_LOCK
                - SYS_RAWIO
              runAsGroup: 0
              runAsUser: 0
            imagePullPolicy: Always
            command:
              - /bin/bash
              - -c
            args:
              - |-
                # Clear /dev/shm on start to prevent running out of space when crashes occur
                # https://github.com/llm-d/llm-d/issues/352
                find /dev/shm -type f -delete

                LEADER_PCLQ_NAME="${GROVE_PCLQ_NAME/worker/leader}"
                LEADER_ADDRESS="${LEADER_PCLQ_NAME}-0.${GROVE_HEADLESS_SERVICE}"

                exec vllm serve \
                  /models/deepseek-r1-fp4 \
                  --port 8000 \
                  --trust-remote-code \
                  --disable-uvicorn-access-log \
                  --enable-expert-parallel \
                  --tensor-parallel-size $TP_SIZE \
                  --data-parallel-size $DP_SIZE_LOCAL \
                  --data-parallel-size-local $DP_SIZE_LOCAL \
                  --data-parallel-address ${LEADER_ADDRESS} \
                  --data-parallel-rpc-port 5555 \
                  --kv_transfer_config '{"kv_connector":"NixlConnector",
                                          "kv_role":"kv_both"}' \
                  --async-scheduling \
                  --kv-cache-memory-bytes=${KV_CACHE_MEMORY_BYTES-}
            env:
            - name: DP_SIZE_LOCAL
              value: "4"
            - name: TP_SIZE
              value: "1"
            - name: TRITON_LIBCUDA_PATH
              value: /usr/lib64
            - name: VLLM_SKIP_P2P_CHECK
              value: "1"
            - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
              value: "1"
            - name: VLLM_USE_DEEP_GEMM
              value: "1"
            - name: VLLM_ALL2ALL_BACKEND
              value: flashinfer_all2allv
            - name: VLLM_USE_FLASHINFER_MOE_FP4
              value: "1"
            - name: VLLM_FLASHINFER_MOE_BACKEND
              value: throughput
            - name: NCCL_MNNVL_ENABLE
              value: "1"
            - name: NCCL_CUMEM_ENABLE
              value: "1"
            - name: NCCL_NVLS_ENABLE
              value: "1"
            - name: VLLM_USE_NCCL_SYMM_MEM
              value: "1"
            - name: GLOO_SOCKET_IFNAME
              value: eth0
            - name: NCCL_SOCKET_IFNAME
              value: eth0
            - name: TORCH_DISTRIBUTED_TIMEOUT
              value: "7200"
            - name: NCCL_STORE_TIMEOUT
              value: "7200"
            - name: TORCH_DIST_INIT_BARRIER_TIMEOUT
              value: "7200"
            - name: VLLM_ENGINE_READY_TIMEOUT_S
              value: "1800"
            - name: VLLM_NIXL_SIDE_CHANNEL_HOST
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP

            # Use cache directories from the mounted volume under one easy to mount
            # root directory.
            - name: CUDA_CACHE_PATH
              value: /var/cache/vllm/cuda
            - name: CCACHE_DIR
              value: /var/cache/vllm/ccache
            - name: VLLM_CACHE_ROOT
              value: /var/cache/vllm/vllm
            - name: FLASHINFER_WORKSPACE_BASE
              value: /var/cache/vllm/flashinfer
            # HuggingFace is likely to be quite a bit larger, give it its own cache
            - name: HF_HUB_CACHE
              value: /var/cache/huggingface

            ports:
            - containerPort: 8000
              name: metrics
              protocol: TCP
            startupProbe:
              httpGet:
                path: /health
                port: metrics
              initialDelaySeconds: 0
              periodSeconds: 10
              timeoutSeconds: 5
              failureThreshold: 1800
            livenessProbe:
              httpGet:
                path: /health
                port: metrics
              periodSeconds: 30
              timeoutSeconds: 5
              failureThreshold: 3
            readinessProbe:
              httpGet:
                path: /v1/models
                port: metrics
              periodSeconds: 10
              timeoutSeconds: 5
              failureThreshold: 3
            resources:
              limits:
                ephemeral-storage: 1Ti
                memory: 512Gi
                nvidia.com/gpu: "4"
              requests:
                ephemeral-storage: 1Ti
                memory: 512Gi
                nvidia.com/gpu: "4"
            volumeMounts:
              - name: dshm
                mountPath: /dev/shm
              - name: hf-cache
                mountPath: /var/cache/huggingface
              - name: jit-cache
                mountPath: /var/cache/vllm
    - name: decode-leader
      labels:
        kai.scheduler/queue: default-queue
        llm-d.ai/inference-serving: "true"
        llm-d.ai/guide: "wide-ep"
        llm-d.ai/accelerator-variant: "gpu"
        llm-d.ai/accelerator-vendor: "nvidia"
        llm-d.ai/model: DeepSeek-R1-0528
        llm-d.ai/role: decode
      spec:
        replicas: 1
        minAvailable: 1
        roleName: decode-leader
        podSpec:
          serviceAccountName: deepseek-r1
          schedulerName: kai-scheduler
          initContainers:
            - name: routing-proxy
              args:
                - --port=8000
                - --vllm-port=8200
                - --connector=nixlv2
                - -v=1
                - --secure-proxy=false
              image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.5.0
              imagePullPolicy: Always
              ports:
                - containerPort: 8000
                  name: sidecar
                  protocol: TCP
              resources: {}
              restartPolicy: Always
              securityContext:
                allowPrivilegeEscalation: false
                runAsNonRoot: true
          volumes:
            - name: dshm
              emptyDir:
                medium: Memory
                sizeLimit: 2Gi
            - name: hf-cache
              emptyDir: {}
            - name: jit-cache
              emptyDir: {}
          containers:
          - name: vllm
            image: ghcr.io/llm-d/llm-d-cuda:v0.5.0
            securityContext:
              capabilities:
                add:
                - IPC_LOCK
                - SYS_RAWIO
              runAsGroup: 0
              runAsUser: 0
            imagePullPolicy: Always
            command:
              - /bin/bash
              - -c
            args:
              - |-
                # Clear /dev/shm on start to prevent running out of space when crashes occur
                # https://github.com/llm-d/llm-d/issues/352
                find /dev/shm -type f -delete

                START_RANK=0
                LEADER_PCLQ_NAME="${GROVE_PCLQ_NAME/worker/leader}"
                LEADER_ADDRESS="${LEADER_PCLQ_NAME}-0.${GROVE_HEADLESS_SERVICE}"

                exec vllm serve \
                  /models/deepseek-r1-fp4 \
                  --port 8200 \
                  --trust-remote-code \
                  --disable-uvicorn-access-log \
                  --data-parallel-hybrid-lb \
                  --enable-expert-parallel \
                  --tensor-parallel-size $TP_SIZE \
                  --data-parallel-size $((GROVE_PCSG_TEMPLATE_NUM_PODS * DP_SIZE_LOCAL)) \
                  --data-parallel-size-local $DP_SIZE_LOCAL \
                  --data-parallel-address ${LEADER_ADDRESS} \
                  --data-parallel-rpc-port 5555 \
                  --data-parallel-start-rank $START_RANK \
                  --kv_transfer_config '{"kv_connector":"NixlConnector",
                                          "kv_role":"kv_both"}' \
                  --async-scheduling \
                  --compilation_config '{"cudagraph_mode": "FULL_DECODE_ONLY"}' \
                  --kv-cache-memory-bytes=${KV_CACHE_MEMORY_BYTES-}
            env:
            - name: DP_SIZE_LOCAL
              value: "4"
            - name: TP_SIZE
              value: "1"
            - name: TRITON_LIBCUDA_PATH
              value: /usr/lib64
            - name: VLLM_SKIP_P2P_CHECK
              value: "1"
            - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
              value: "1"
            - name: VLLM_USE_DEEP_GEMM
              value: "1"
            - name: VLLM_ALL2ALL_BACKEND
              value: flashinfer_all2allv
            - name: VLLM_USE_FLASHINFER_MOE_FP4
              value: "1"
            - name: VLLM_FLASHINFER_MOE_BACKEND
              value: throughput
            - name: VLLM_MLA_FP8_PROJ
              value: "1"
            - name: VLLM_DEEPEP_LOW_LATENCY_ALLOW_NVLINK
              value: "1"
            - name: VLLM_DEEPEP_LOW_LATENCY_USE_MNNVL
              value: "1"
            - name: VLLM_DEEPEP_BUFFER_SIZE_MB
              value: "0"
            - name: VLLM_EP_USE_SBO
              value: "0"
            - name: VLLM_DEEPEPLL_NVFP4_DISPATCH
              value: "1"
            - name: VLLM_ENABLE_FUSED_MOE_ACTIVATION_CHUNKING
              value: "0"
            - name: VLLM_V1_OUTPUT_PROC_CHUNK_SIZE
              value: "2048"
            - name: NCCL_MNNVL_ENABLE
              value: "1"
            - name: NCCL_CUMEM_ENABLE
              value: "1"
            - name: NCCL_NVLS_ENABLE
              value: "1"
            - name: VLLM_USE_NCCL_SYMM_MEM
              value: "1"
            - name: GLOO_SOCKET_IFNAME
              value: eth0
            - name: NCCL_SOCKET_IFNAME
              value: eth0
            - name: TORCH_DISTRIBUTED_TIMEOUT
              value: "7200"
            - name: NCCL_STORE_TIMEOUT
              value: "7200"
            - name: TORCH_DIST_INIT_BARRIER_TIMEOUT
              value: "7200"
            - name: VLLM_NIXL_SIDE_CHANNEL_HOST
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: VLLM_NIXL_SIDE_CHANNEL_PORT
              value: "5700"
            - name: VLLM_NIXL_ABORT_REQUEST_TIMEOUT
              value: "600"
            - name: VLLM_ENGINE_READY_TIMEOUT_S
              value: "1800"

            # Use cache directories from the mounted volume under one easy to mount
            # root directory.
            - name: CUDA_CACHE_PATH
              value: /var/cache/vllm/cuda
            - name: CCACHE_DIR
              value: /var/cache/vllm/ccache
            - name: VLLM_CACHE_ROOT
              value: /var/cache/vllm/vllm
            - name: FLASHINFER_WORKSPACE_BASE
              value: /var/cache/vllm/flashinfer
            # HuggingFace is likely to be quite a bit larger, give it its own cache
            - name: HF_HUB_CACHE
              value: /var/cache/huggingface

            ports:
            - containerPort: 8200
              name: metrics
              protocol: TCP
            startupProbe:
              httpGet:
                path: /health
                port: metrics
              initialDelaySeconds: 0
              periodSeconds: 10
              timeoutSeconds: 5
              failureThreshold: 1800
            livenessProbe:
              httpGet:
                path: /health
                port: metrics
              periodSeconds: 30
              timeoutSeconds: 5
              failureThreshold: 3
            readinessProbe:
              httpGet:
                path: /v1/models
                port: metrics
              periodSeconds: 10
              timeoutSeconds: 5
              failureThreshold: 3
            resources:
              limits:
                ephemeral-storage: 1Ti
                memory: 512Gi
                nvidia.com/gpu: "4"
              requests:
                ephemeral-storage: 1Ti
                memory: 512Gi
                nvidia.com/gpu: "4"
            volumeMounts:
              - name: dshm
                mountPath: /dev/shm
              - name: hf-cache
                mountPath: /var/cache/huggingface
              - name: jit-cache
                mountPath: /var/cache/vllm
    - name: decode-worker
      labels:
        kai.scheduler/queue: default-queue
        llm-d.ai/inference-serving: "true"
        llm-d.ai/guide: "wide-ep"
        llm-d.ai/accelerator-variant: "gpu"
        llm-d.ai/accelerator-vendor: "nvidia"
        llm-d.ai/model: DeepSeek-R1-0528
        llm-d.ai/role: decode
      spec:
        replicas: 7
        minAvailable: 7
        roleName: decode-worker
        podSpec:
          serviceAccountName: deepseek-r1
          schedulerName: kai-scheduler
          initContainers:
            - name: routing-proxy
              args:
                - --port=8000
                - --vllm-port=8200
                - --connector=nixlv2
                - -v=1
                - --secure-proxy=false
              image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.5.0
              imagePullPolicy: Always
              ports:
                - containerPort: 8000
                  name: sidecar
                  protocol: TCP
              resources: {}
              restartPolicy: Always
              securityContext:
                allowPrivilegeEscalation: false
                runAsNonRoot: true
          volumes:
            - name: dshm
              emptyDir:
                medium: Memory
                sizeLimit: 2Gi
            - name: hf-cache
              emptyDir: {}
            - name: jit-cache
              emptyDir: {}
          containers:
          - name: vllm
            image: ghcr.io/llm-d/llm-d-cuda:v0.5.0
            securityContext:
              capabilities:
                add:
                - IPC_LOCK
                - SYS_RAWIO
              runAsGroup: 0
              runAsUser: 0
            imagePullPolicy: Always
            command:
              - /bin/bash
              - -c
            args:
              - |-
                # Clear /dev/shm on start to prevent running out of space when crashes occur
                # https://github.com/llm-d/llm-d/issues/352
                find /dev/shm -type f -delete

                START_RANK=$(( ( ${GROVE_PCLQ_POD_INDEX:-0} + 1 ) * DP_SIZE_LOCAL ))
                LEADER_PCLQ_NAME="${GROVE_PCLQ_NAME/worker/leader}"
                LEADER_ADDRESS="${LEADER_PCLQ_NAME}-0.${GROVE_HEADLESS_SERVICE}"
              
                exec vllm serve \
                  /models/deepseek-r1-fp4 \
                  --port 8200 \
                  --trust-remote-code \
                  --disable-uvicorn-access-log \
                  --data-parallel-hybrid-lb \
                  --enable-expert-parallel \
                  --tensor-parallel-size $TP_SIZE \
                  --data-parallel-size $((GROVE_PCSG_TEMPLATE_NUM_PODS * DP_SIZE_LOCAL)) \
                  --data-parallel-size-local $DP_SIZE_LOCAL \
                  --data-parallel-address ${LEADER_ADDRESS} \
                  --data-parallel-rpc-port 5555 \
                  --data-parallel-start-rank $START_RANK \
                  --kv_transfer_config '{"kv_connector":"NixlConnector",
                                          "kv_role":"kv_both"}' \
                  --async-scheduling \
                  --compilation_config '{"cudagraph_mode": "FULL_DECODE_ONLY"}' \
                  --kv-cache-memory-bytes=${KV_CACHE_MEMORY_BYTES-}
            env:
            - name: DP_SIZE_LOCAL
              value: "4"
            - name: TP_SIZE
              value: "1"
            - name: TRITON_LIBCUDA_PATH
              value: /usr/lib64
            - name: VLLM_SKIP_P2P_CHECK
              value: "1"
            - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
              value: "1"
            - name: VLLM_USE_DEEP_GEMM
              value: "1"
            - name: VLLM_ALL2ALL_BACKEND
              value: flashinfer_all2allv
            - name: VLLM_USE_FLASHINFER_MOE_FP4
              value: "1"
            - name: VLLM_FLASHINFER_MOE_BACKEND
              value: throughput
            - name: VLLM_MLA_FP8_PROJ
              value: "1"
            - name: VLLM_DEEPEP_LOW_LATENCY_ALLOW_NVLINK
              value: "1"
            - name: VLLM_DEEPEP_LOW_LATENCY_USE_MNNVL
              value: "1"
            - name: VLLM_DEEPEP_BUFFER_SIZE_MB
              value: "0"
            - name: VLLM_EP_USE_SBO
              value: "0"
            - name: VLLM_DEEPEPLL_NVFP4_DISPATCH
              value: "1"
            - name: VLLM_ENABLE_FUSED_MOE_ACTIVATION_CHUNKING
              value: "0"
            - name: VLLM_V1_OUTPUT_PROC_CHUNK_SIZE
              value: "2048"
            - name: NCCL_MNNVL_ENABLE
              value: "1"
            - name: NCCL_CUMEM_ENABLE
              value: "1"
            - name: NCCL_NVLS_ENABLE
              value: "1"
            - name: GLOO_SOCKET_IFNAME
              value: eth0
            - name: NCCL_SOCKET_IFNAME
              value: eth0
            - name: TORCH_DISTRIBUTED_TIMEOUT
              value: "7200"
            - name: NCCL_STORE_TIMEOUT
              value: "7200"
            - name: TORCH_DIST_INIT_BARRIER_TIMEOUT
              value: "7200"
            - name: VLLM_NIXL_SIDE_CHANNEL_HOST
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: VLLM_NIXL_SIDE_CHANNEL_PORT
              value: "5700"
            - name: VLLM_NIXL_ABORT_REQUEST_TIMEOUT
              value: "600"
            - name: VLLM_ENGINE_READY_TIMEOUT_S
              value: "1800"

            # Use cache directories from the mounted volume under one easy to mount
            # root directory.
            - name: CUDA_CACHE_PATH
              value: /var/cache/vllm/cuda
            - name: CCACHE_DIR
              value: /var/cache/vllm/ccache
            - name: VLLM_CACHE_ROOT
              value: /var/cache/vllm/vllm
            - name: FLASHINFER_WORKSPACE_BASE
              value: /var/cache/vllm/flashinfer
            # HuggingFace is likely to be quite a bit larger, give it its own cache
            - name: HF_HUB_CACHE
              value: /var/cache/huggingface

            ports:
            - containerPort: 8200
              name: metrics
              protocol: TCP
            startupProbe:
              httpGet:
                path: /health
                port: metrics
              initialDelaySeconds: 0
              periodSeconds: 10
              timeoutSeconds: 5
              failureThreshold: 1800
            livenessProbe:
              httpGet:
                path: /health
                port: metrics
              periodSeconds: 30
              timeoutSeconds: 5
              failureThreshold: 3
            readinessProbe:
              httpGet:
                path: /v1/models
                port: metrics
              periodSeconds: 10
              timeoutSeconds: 5
              failureThreshold: 3
            resources:
              limits:
                ephemeral-storage: 1Ti
                memory: 512Gi
                nvidia.com/gpu: "4"
              requests:
                ephemeral-storage: 1Ti
                memory: 512Gi
                nvidia.com/gpu: "4"
            volumeMounts:
              - name: dshm
                mountPath: /dev/shm
              - name: hf-cache
                mountPath: /var/cache/huggingface
              - name: jit-cache
                mountPath: /var/cache/vllm
    podCliqueScalingGroups:
    - name: decode-sg
      cliqueNames: [decode-leader, decode-worker]
      replicas: 1
      minAvailable: 1